# DeepResearch 如何做系统化评估


先说结论：“deep research 项目” 要系统化评估，核心是把它当成 “一个多阶段的研究型 Agent 系统”，分 3 层来看：结果质量、研究过程、对业务任务的真实贡献，然后配一套基准任务集 + 自动评估 + 人评 + 在线指标闭环。我给你一个可以直接落地的评估框架，你可以按这个去搭 pipeline。
下面是几个不错的文章或者数据集：
- [DeepResearch Bench](https://deepresearch-bench.github.io/)
- [Hugging Face: Open Deep Research](https://huggingface.co/blog/open-deep-research)
- [ACM Digital Library](https://dl.acm.org/doi/10.1145/3626772.3661346)
- [Orq.ai Multi-Agent LLM Eval System](https://orq.ai/blog/multi-agent-llm-eval-system)
- [Live Science Technology](https://www.livescience.com/technology)
- [Medium](https://medium.com/)
- [Anthropic: Multi-Agent Research System](https://www.anthropic.com/engineering/multi-agent-research-system)

## 1. 先把问题说清楚：你到底在评估什么？
- 对 deepresearch 系统，一般有三类目标：
- 研究质量
  - 结论是否正确、不瞎编
  - 信息是否覆盖全面，不漏关键信息
  - 是否给出了可信引用
- 研究过程
  - 搜索 / 浏览是否有 breadth & depth
  - 有没有认真比对不同来源，而不是只看前两条链接
  - 推理链条是否合理，有无自我修正
- 业务效果
  - 这个研究结果，对真实业务决策 / 写方案 / 写代码，有没有明显帮助
  - 人类专家是否愿意直接用 / 稍作修改后用
  - 和 “人肉调研 + 自己写” 的方式相比，节省了多少时间 / 成本
- 系统化评估，就是把这 3 层拆成可量化指标 + 固定流程。

## 2. 构建一套 “深度研究基准任务集”(Benchmark)
你可以参考 DeepResearch Bench 这种思路：用一批真实世界的复杂研究问题来测 DeepResearch Agent。

### 2.1 任务设计原则
- 建议先做一个 50-200 道题的题库，每道题是一条“需要上网翻很多页、整合观点”的问题，比如：
  - 某赛道 / 公司的竞争格局、关键玩家、未来 3 年趋势
  - 某种技术路径（比如 RLVR、长上下文缓存方案）的各家方案对比、trade-off
  - 某行业监管政策演进 + 对业务影响分析
- 维度上标注清楚：
  - 领域：大模型、销售、医疗、金融、教育……
  - 难度：基础 / 中级 / 高级（是否需要跨领域、多来源推理）
  - 任务类型：
    - 纯事实聚合（Fact-heavy）
    - 观点 / 方案对比（Compare & Contrast）
    - 战略 / 路线规划（Planning）
    - 风险评估（Risk & Safety）

### 2.2 参考答案 / 参考标准
- 专家写一份“Gold Report”
  - 内容：结构化报告 + 关键结论 + 关键证据列表（sources）
  - 用于对齐大方向和关键点
- 或只给“关键事实 / 关键要点列表”
  - 列出：这道题必须覆盖的 top-K facts、关键风险、必须引用的几个权威来源
- 模型输出再用“覆盖率 + 正确率”去算分

DeepResearch Bench 里就是通过“报告质量 + 检索 & 引用质量”两套框架来评估的（RACE + FACT）。

## 3. 结果层面的系统化指标（对 “研究报告” 打分）
可以设计一份统一的评分 Rubric，让 LLM-judge + 人类评审共用：

### 3.1 核心维度（建议都打 1-5 分）
1. 事实正确性 / 幻觉率
   - 抽取报告里的关键陈述（尤其是数字、时间、因果关系）
   - 用独立检索 + 人审，标记为：正确 / 有争议 / 错误 / 无法验证
   - 指标：
     - Fact_Correct_Rate = 正确事实数 / 可验证事实数
     - Hallucination_Rate = 明确错误 + 无出处支撑的关键陈述 / 总陈述数
2. 覆盖度 & 深度（Coverage & Depth）
   - 把 Gold 标准里的关键点当作集合 G，模型报告覆盖的记为 C：
   - Recall = |C| / |G|（有没有漏最关键的东西）
   - 深度可以看：是否能指出不同观点 / 方案的优缺点，而不是 “堆材料”
3. 结构化程度 & 可用性
   - 结构是否清晰：目录、分节、结论先行、要点摘要
   - 是否给出可直接执行的 action items / 方案建议
4. 引用质量 & 溯源性
   - 每个关键结论是否附带来源
   - 来源是否来自多个域名、多种类型（论文、官方文档、权威媒体）
   - 是否有 “只看 SEO 垃圾站” 的问题
5. 风险与偏见控制
   - 是否过度简化、过度外推（特别是医学 / 金融等领域）(Live Science)
   - 是否明确指出不确定性与局限

这些维度都可以写成一个统一 JSON Schema，例如：
```json
{
  "overall_score": "0-10",
  "dimensions": {
    "factuality": "0-5",
    "coverage": "0-5",
    "depth": "0-5",
    "structure": "0-5",
    "citation_quality": "0-5",
    "risk_awareness": "0-5"
  },
  "feedback": {
    "strengths": ["..."],
    "weaknesses": ["..."],
    "missing_points": ["..."]
  }
}
```
然后用一个独立模型做 LLM-as-a-judge 自动打分，再抽样做人评校准。(ACM Digital Library)

## 4. 过程层面的评估（对 “Agent 行为” 打分）
Deepresearch 项目往往是多 Agent + 多工具的 pipeline，例如：Planner → Researcher → Critic → Writer。除了最终报告，你还需要盯这些过程指标：

1. 搜索 & 浏览质量
   - 平均搜索次数 / 工具调用次数
   - 访问的独立域名数
   - 平均每个 SERP 深度（只点前 3 条，还是会翻页）
   - 对 “低质量站点” 的访问比例
2. 探索 vs 利用（Breadth vs Depth）
   - 是不是只看第一批结果就停了？
   - 对重要来源有没有深入阅读、对比？
3. 推理 / 协同行为
   - Planner 是否能把子任务拆对（有无多余子任务 / 漏掉关键子任务）
   - Agents 之间有没有复用彼此发现，还是各自为战？(orq.ai)
4. 自我检查 & 修正能力
   - 是否在末尾做 consistency check /conflict detection
   - 是否主动指出：“某某问题数据冲突，需要人工进一步核查”

这些东西可以通过分析系统日志来统计，最后汇总成指标 Dashboard，比如：
- avg_num_search_calls
- avg_unique_domains
- avg_depth_per_query
- planner_error_rate（子任务设计不合理需要重试的比例）

## 5. 业务效果层面的评估（真正关心的那一层）
Deepresearch 最终是要服务具体项目：战略研究、竞品分析、销售策略、技术选型……这里建议直接用在线实验 + 人类工作流指标：

- 时间 & 成本
  - 同样一类调研任务，让人类分组：
    - A 组：完全人工
    - B 组：使用 deepresearch 提前生成报告，再由人类审阅修改
  - 比较：
    - 任务完成时间
    - 人力成本
    - 需要的“额外查证次数”
- 决策质量 / 方案质量
  - 例如产品立项评审、销售方案评审时，让评委盲测：
  - 只看“人工版方案” vs “deepresearch + 人工微调版方案”
  - 打分：创新性、完整度、风险意识、可执行性
- 用户满意度 & 复用率
  - 让内部用户对每份报告打分 (1-5)
  - 统计某份 deepresearch 报告后续被引用、转发、改写成 PPT 的次数

这部分才是“这个系统值不值得继续投”的硬指标。(Medium)

## 6. 实施路线图：你可以按这个节奏搭评估系统

- Step 1：定义 3-5 个典型场景
  - 例如：
    - 新行业 / 新产品的快速研报
    - 某技术路径的技术栈对比和选型建议
    - 某政策 / 监管变动对业务影响分析
  - 每个场景选 20-50 个真实问题，先做成 v0 题集
- Step 2：为一部分题做“高质量参考”
  - 对最关键的 10-20 题，找 domain expert 写 Gold Report
  - 对其它题，至少标注一个“关键 fact & 关键结论列表”
- Step 3：搭一个 evaluation harness（统一评测脚本 / workflow）
  - 输入：问题 + 评测配置
  - 调用你的 deepresearch pipeline，产生完整报告 + 过程日志
  - 对每个报告：
    - 调用独立 LLM-judge，根据统一 rubric 打自动分
    - 抽样给人类做双盲打分（对齐标尺）
  - 将所有分数 + 日志指标入库（例如 ClickHouse / Postgres + BI 图表）
- Step 4：形成“版本回归测试”
  - 每次你改：
    - 模型版本
    - Agent 数量 / 协作策略
    - 检索策略 / 工具集
  - 都在同一批题上跑一次评测，对比：
    - 结果质量（分数变化）
    - 过程指标（搜索次数、工具调用、成本、时延）
  - 把评测跑成 CI/CD 的一环（类 ML 模型回归测试）
- Step 5（可选）：把评估变成 Reward Signal
  - 如果要做 RL / 偏好对齐，可以用：
    - 人类对报告的成对偏好（A 报告 vs B 报告）
    - 自动评估的维度分数
  - 组合训练“Research-Report Reward Model”，专门给 deepresearch 输出打分，用于：
    - 训练时：RLHF / RLAIF
    - 推理时：在多个候选报告中选一个分数更高的
  - 参考：与评估法律文书质量的领域评价框架类似（换成“研究报告领域”）（arXiv）

## 7. 实操（交通、金融、法律、餐饮）

分三层看：结果质量 → 研究过程 → 真实业务价值，同时把「行业维度」带进去。

### 7.1 先把“你这个助手”在做什么说清楚
- 行业 & 细分赛道研究
  - 行业现状、趋势、核心玩家、竞争格局
- 政策 / 监管 / 法规解读
  - 新规影响、合规要求、风险点
- 方案 / 策略设计
  - 进入策略、定价策略、运营优化方案
- 案例 & 风险分析
  - 典型成功 / 失败案例总结 + 教训 + 可复制要点

后面所有评估，都建立在「行业 × 任务类型」这个二维矩阵上。

### 7.2 搭一个「行业 × 任务类型」基准任务矩阵
你可以给每个行业设计一批典型任务，形成一个 Benchmark 表：
#### 基准任务矩阵表
| 行业 | 任务类型 | 示例任务（让模型做什么） |
| --- | --- | --- |
| 交通 | 政策 & 合规 | 评估某城市网约车 / 货运新规对平台运营的影响，并给出合规改造建议 |
| 交通 | 运营优化 | 为城市公共交通运营商设计早晚高峰运力优化方案 |
| 金融 | 行业 & 细分赛道 | 对消费金融 / 供应链金融某细分赛道做格局 + 风险 + 监管趋势分析 |
| 金融 | 产品 & 风控策略 | 对某类贷款产品的目标客群、定价思路、风控要点做评估 |
| 法律 | 法规 / 案例研判 | 解析某一类合同纠纷的典型判决逻辑，提炼实务要点与风险条款 |
| 法律 | 合同 / 制度优化建议 | 针对某企业现有合同 / 制度，提出风险点与修改建议（注意只能 “研判 + 提示”，不能充当律师） |
| 餐饮 | 选址 & 门店策略 | 为连锁餐饮品牌评估是否进入某商圈 / 高铁站店，给出决策依据与风险 |
| 餐饮 | 运营 & 品类策略 | 分析某城市目标客群，设计品类组合、价格带和营销打法 |

#### 每行任务配套要素
- 输入规范：
  - 背景信息（企业现状、目标市场、约束条件）
  - 提供的已有资料（历史数据、公开信息、公司内部文档）
  - 模型允许访问的外部搜索 / 数据源
- 期望输出结构（模板化）：
  - 建议用统一 JSON/Markdown 模板，比如：
    - 概览 & 结论先行
    - 关键事实 & 数据
    - 分析与推理
    - 风险 & 不确定性

### 7.3 结果层评估：通用维度 + 行业特定要求
#### 通用维度（0-5 分，所有行业适用）
- 事实正确性 & 幻觉控制
  - 关键数据、时间、法律条款、监管机构、案例是否真实存在
  - 对不能确定的地方是否标注“不确定 / 需进一步核实”
- 覆盖度 & 视角完整性
  - 是否覆盖该问题下必须提到的关键要素
  - 是否存在严重漏项（如讲网约车监管却未提到主管部门 / 关键条例）
- 逻辑与推理质量
  - 结论是否能从给出的事实与推理链条顺出来
  - 是否存在明显跳跃、拍脑袋式判断
- 结构化程度 & 可用性
  - 是否结论先行、结构清晰，便于直接用于 PPT / 决策备忘录
  - 建议是否具备可执行性（时间表 / 优先级 / 资源假设）
- 引用 & 溯源
  - 关键结论是否附带来源信息
  - 来源是否多样（官方 / 权威 / 专业机构 / 论文等），而非纯 SEO 站
  - 是否清晰区分“事实引用”与“模型整理和判断”
打分输出可以统一成：
```json
{
  "overall_score": "0-10",
  "dimensions": {
    "factuality": "0-5",
    "coverage": "0-5",
    "reasoning": "0-5",
    "structure": "0-5",
    "citation": "0-5"
  },
  "fatal_errors": [],
  "notes": {
    "strengths": ["..."],
    "weaknesses": ["..."],
    "missing_points": ["..."]
  }
}
```
由「独立 LLM-judge + 人类专家抽样」共同完成。

#### 金融行业
- 红线点：
  - 给出类似“买这个 XXX 基金肯定赚钱”“建议你加杠杆买入”等个体投资建议
  - 误导性 / 过度确定性表达，忽视风险披露
- 附加维度：
  - 风险意识：是否充分提示信用风险、流动性风险、合规风险
  - 合规 & 监管对齐：是否提及适用监管框架、持牌要求、信息披露要求等
- 强制要求：
  - 每个金融任务的输出必须含“风险提示”小节；缺失则直接扣至 0-1 分

#### 法律行业
- 红线点：
  - 直接给出“你应该签 / 不该签”“诉讼肯定能赢”这类确定性法律建议
  - 捏造不存在的法条、案例、法院判决
- 附加维度：
  - 引用严谨性：引用条文 / 案例是否准确、是否标明法域与时间（施行版本）
  - 保留意见：对分歧观点 / 裁判趋势是否提供不同视角，而非“一锤定音”
- 强制要求：
  - 输出中必须出现“本解读不构成正式法律意见，具体应咨询执业律师”等免责声明；否则视为不合规

#### 餐饮行业
- 红线点：
  - 明显无视卫生 / 食品安全法规（如鼓励违规改造后厨、标识造假）
- 附加维度：
  - 落地性：建议是否考虑采购 / 供应链 / 人力成本，而非只谈“概念玩法”
  - 经济性：是否有 ROI 的基本测算或至少给出计算思路（客单价、坪效、翻台率）

### 7.4 过程层评估：看“研究行为”是否像合格咨询顾问
- 信息源类型覆盖
  - 交通：是否访问交通部 / 交通委 / 城市交通局官网、官方统计年鉴等
  - 金融：是否包含监管机构（央行、银保监、证监、交易所）等权威站点
  - 法律：是否访问官网法条库、权威判决数据库，而非仅看知乎 / 论坛
  - 餐饮：是否参考行业报告、连锁品牌年报、第三方调研机构
- Breadth & Depth 指标
  - 平均搜索次数、平均 unique 域名数
  - 是否只停留在 SERP 第一页 / 前三条
  - 重要来源是否有“深入阅读”（多次点击 / 多段引用）
- Agent 协作质量（多 Agent 架构适用）
  - Planner 拆出来的子任务数量 & 覆盖度
  - Researcher 是否逐条完成子任务，而非胡乱搜索
  - Critic / Reviewer 是否提出修改意见（指出信息冲突 / 不确定性）
- 自我质疑与不确定性表达
  - 日志中是否出现“source A 与 source B 观点相反，可能由于时间 / 口径不同”等自我校验
  - 输出是否给出“不确定点列表”，而非装作全部确定
- 过程指标落库与看板
  - 为每个任务记录一条「过程指标 JSON」，在 BI 报表中观察：
    - 某个版本升级后，信息源多样性是否提升
    - 搜索习惯是否越来越“懒”
    - Critic Agent 参与度是否下降
